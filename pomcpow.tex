\chapter{Online Algorithms for Continuous POMDPs}

\section{Background}

Although the research surveyed in \cref{sec:solutions} has yielded effective solution techniques for many classes of POMDPs, there remains a need for simple, general purpose online solvers that can handle continuous spaces, especially continuous observation spaces.
This chapter takes some first steps toward addressing that need.

POMCP and other online methods can easily accomodate continuous state spaces without any modification \cite{goldhoorn2014continuous}.
However, there has been less progress on problems with continuous observation spaces.
This chapter presents two similar algorithms which address the challenge of solving POMDPs with continuous state, action, and observation spaces.
The first is based on POMCP and is called partially observable Monte Carlo planning with observation widening (POMCPOW). 
The second solves the belief-space MDP and is called particle filter trees with double progressive widening (PFT-DPW).

\begin{figure}[htb]
\begin{center}
    \includegraphics[width=0.9\columnwidth]{media/continuous_tree.pdf}
\end{center}
\caption[POMCP tree on a continuous observation space]{POMCP tree for a discrete POMDP (left), and for a POMDP with a continuous observation space (right). Because the observation space is continuous, each simulation creates a new observation node and the tree cannot extend deeper.}
\label{fig:ctree}
\end{figure}

There are two challenges that make tree search difficult in continuous spaces.
The first is that, since the probability of sampling the same real number twice from a continuous random variable is zero, the width of the planning trees explodes on the first step, causing them to be too shallow to be useful (see \cref{fig:ctree}).
POMCPOW and PFT-DPW resolve this issue with a technique called double progressive widening (DPW) \cite{couetoux2011double}. 
The second issue is that, even when DPW is applied, the belief representations used by current solvers collapse to a single state particle, resulting in overconfidence.
As a consequence, the solutions obtained resemble QMDP policies, and there is no incentive for information gathering.
POMCPOW and PFT-DPW overcome this issue by using the observation model to weight the particles used to represent beliefs.

A small amount of previous research has sought online solutions to continuous POMDPs.
ABT has been extended to use generalized pattern search for selecting locally optimal continuous actions, an approach which is especially effective in problems where high precision is important \cite{seiler2015online}.
Continuous observation Monte Carlo tree search (COMCTS) constructs observation classification trees to automatically partition the observation space in a POMCP-like approach, however it did not perform much better than a Monte Carlo rollout approach in experiments \cite{pas2012simulation}.

\section{Algorithms}

This section presents and discusses several methods for handling continuous POMDPs.
First, it presents three new MCTS-DPW-based algorithms.
The first of these is POMCP-DPW.
\Cref{thm:qmdp} shows that this algorithm is suboptimal in some cases.
The second algorithm is PFT-DPW, a straightforward application of MCTS-DPW to the belief MDP with particle filters to approximate belief updates.
The third algorithm is POMCPOW, which fixes the problems noted about POMCP-DPW and successfully extends POMCP to problems with continuous observation spaces.
These three algorithms are followed by brief discussion about the discretization alternative to these algorithms and the additional observation distribution requirement of POMCPOW and PFT-DPW.

The three new algorithms in this chapter share a common structure.
For all algorithms, the entry point for the decision making process is the \textproc{Plan} procedure, which takes the current belief, $b$, as an input (\textproc{Plan} differs slightly for PFT-DPW in \cref{alg:pft}).
The algorithms also share the same \textproc{ActionProgWiden} function to control progressive widening of the action space.
These components are listed in Listing \ref{alg:common}.
The difference between the algorithms is in the \textproc{Simulate} function.

The following variables are used in the listings and text:
$h$ represents a history $(b, a_1, o_1, \dots a_k, o_k)$, and $ha$ and $hao$ are shorthand for histories with $a$ and $(a,o)$ appended to the end, respectively;
$d$ is the depth to explore, with $d_\text{max}$ the maximum depth;
$C$ is a list of the children of a node (along with the reward in the case of PFT-DPW);
$N$ is a count of the number of visits; and $M$ is a count of the number of times that a history has been generated by the model.
The list of states associated with a node is denoted $B$, and $W$ is a list of weights corresponding to those states.
Finally, $Q(ha)$ is an estimate of the value of taking action $a$ after observing history $h$.
$C$, $N$, $M$, $B$, $W$, and $Q$ are all implicitly initialized to \num{0} or $\emptyset$.
The \textproc{Rollout} procedure, runs a simulation with a default rollout policy, which can be based on the history or fully observed state, for $d$ steps and returns the discounted reward.

% TODO algorithm vs listing numbering
\begin{algorithm}[htbp]
    \floatname{algorithm}{Listing}
    \caption{Common procedures} \label{alg:common}
    \begin{algorithmic}[1]
        \Procedure{Plan}{$b$}
            \For{$i \in 1:n$}
                \State $s \gets \text{sample from }b$
                \State $\Call{Simulate}{s, b, d_\text{max}}$
            \EndFor
            \State $\textbf{return } \underset{a}{\argmax}\, Q(ba)$
        \EndProcedure

        \Procedure {ActionProgWiden}{$h$}
            \If{$|C(h)| \leq k_a N(h)^{\alpha_a}$}
                \State $a \gets \Call{NextAction}{h}$
                \State $C(h) \gets C(h) \cup \{a\}$
            \EndIf
            \State $\textbf{return } \underset{a \in C(h)}{\argmax}\, Q(ha) + c \sqrt{\frac{\log N(h)}{N(ha)}}$
        \EndProcedure

    \end{algorithmic}
\end{algorithm}


\subsection{POMCP-DPW}

The first algorithm that we consider is POMCP with double progressive widening (POMCP-DPW).
In this algorithm, listed in \cref{alg:pomcpdpw}, the number of new children sampled from any node in the tree is limited by DPW using the parameters $k_a$, $\alpha_a$, $k_o$, and $\alpha_o$.
In the case where the simulated observation is rejected (line~\ref{lin:notnew}), the tree search is continued with an observation selected in proportion to the number of times, $M$, it has been previously simulated (line~\ref{lin:selecto}) and a state is sampled from the associated belief (line~\ref{lin:samples}).

\setcounter{algorithm}{0}
\begin{algorithm}[htbp]
    \caption{POMCP-DPW} \label{alg:pomcpdpw}
    \begin{algorithmic}[1]
        \Procedure {Simulate}{$s$, $h$, $d$}        
            \If{$d = 0$}
                \State \textbf{return} $0$
            \EndIf
            \State $a \gets \Call{ActionProgWiden}{h}$
            \If{$|C(ha)| \leq k_o N(ha)^{\alpha_o}$}
                \State $s',o,r \gets G(s,a)$
                \State $C(ha) \gets C(ha) \cup \{o\}$
                \State $M(hao) \gets M(hao) + 1$
                \State $\text{append } s' \text{ to } B(hao)$ \label{lin:insertion}
                \If{$M(hao) = 1$}
                    \State $total \gets r + \gamma \Call{Rollout}{s', hao, d-1}$
                \Else
                    \State $total \gets r + \gamma \Call{Simulate}{s', hao, d-1}$
                \EndIf
            \Else \label{lin:notnew}
                \State $o \gets \text{select } o \in C(ha) \text{ w.p. } \frac{M(hao)}{\sum_{o} M(hao)}$ \label{lin:selecto}
                \State $s' \gets \text{select } s' \in B(hao) \text{ w.p. } \frac{1}{|B(hao)|}$ \label{lin:samples}
                \State $r \gets R(s,a,s')$
                \State $total \gets r + \gamma \Call{Simulate}{s', hao, d-1}$
            \EndIf
            \State $N(h) \gets N(h)+1$
            \State $N(ha) \gets N(ha)+1$
            \State $Q(ha) \gets Q(ha) + \frac{total - Q(ha)}{N(ha)}$
            \State \textbf{return} $total$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

This algorithm obtained remarkably good solutions for a very large autonomous freeway driving POMDP with multiple vehicles (up to 40 continuous fully observable state dimensions and 72 continuous correlated partially observable state dimensions) \cite{sunberg2017value}.
To our knowledge, that is the first work applying progressive widening to POMCP, and it does not contain a detailed description of the algorithm or any theoretical or experimental analysis other than the driving application.

This algorithm may converge to the optimal solution for POMDPs with discrete observation spaces; however, on continuous observation spaces, POMCP-DPW is suboptimal.
In particular, it finds a QMDP policy, that is, the solution under the assumption that the problem becomes fully observable after one time step \cite{littman1995learning,kochenderfer2015decision}.
In fact, for a modified version of POMCP-DPW, it is easy to prove analytically that it will converge to such a policy.
This is expressed formally in Theorem~\ref{thm:qmdp} below.
A complete description of the modified algorithm and problem requirements including the definitions of polynomial exploration, the regularity hypothesis for the problem, and exponentially sure convergence are given in \cref{sec:proof}.

\begin{definition}[QMDP value]
    Let $Q_\text{MDP}(s,a)$ be the optimal state-action value function assuming full observability starting by taking action $a$ in state $s$. The \emph{QMDP value} at belief $b$, $Q_\text{MDP}(b,a)$, is the expected value of $Q_\text{MDP}(s,a)$ when $s$ is distributed according to $b$.
\end{definition}

% \begin{theorem}[Modified POMCP-DPW convergence to QMDP] \label{thm:qmdp}
\begin{restatable}[Modified POMCP-DPW convergence to QMDP]{theorem}{qmdp}
    \label{thm:qmdp}
If a bounded-horizon POMDP meets the following conditions: 1) the state and observation spaces are continuous with a finite observation probability density function, and 2) the regularity hypothesis is met, then modified POMCP-DPW will produce a value function estimate, $\hat{Q}$, that converges to the QMDP value for the problem.
Specifically, there exists a constant $C>0$, such that after $n$ iterations,
\begin{equation*}
    \left| \hat{Q}(b,a) - Q_\text{MDP}(b,a) \right| \leq \frac{C}{n^{1/(10d_{\max}-7)}}
\end{equation*}
exponentially surely in $n$, for every action $a$.
\end{restatable}
% \end{theorem}

A proof of this theorem that leverages work by \citet{auger2013continuous} is given in \cref{sec:proof}, but I will provide a brief justification here.
The key is that belief nodes will contain only a single state particle (see \cref{fig:treecomp}).
This is because, since the observation space is continuous with a finite density function, the generative model will (with probability one) produce a unique observation $o$ each time it is queried.
Thus, for every generated history $h$, only one state will ever be inserted into $B(h)$ (line~\ref{lin:insertion}, \cref{alg:pomcpdpw}), and therefore $h$ is merely an alias for that state. 
Since each belief node corresponds to a state, the solver is actually solving the fully observable MDP at every node except the root node, leading to a QMDP solution.

As a result of \cref{thm:qmdp}, the action chosen by modified POMCP-DPW will match a QMDP policy (a policy of actions that maximize the QMDP value) with high precision exponentially surely (see Corollary 1 of \citet{auger2013continuous}).
For many problems this is a very useful solution,\footnote{Indeed, a useful online QMDP tree search algorithm could be created by deliberately constructing a tree with a single root belief node and fully observable state nodes below it.} but since it neglects the value of information, a QMDP policy is suboptimal for problems where information gathering is important \cite{littman1995learning,kochenderfer2015decision}.
% This phenomenon is demonstrated in simulation in \cref{sec:lightdark}.

Although \cref{thm:qmdp} is only theoretically applicable to the modified version of POMCP-DPW, it helps explain the behavior of other solvers.
Modified POMCP-DPW, POMCP-DPW, DESPOT, and ABT all share the characteristic that a belief node can only contain two states if they generated exactly the same observation.
Since this is an event with zero probability for a continuous observation space, these solvers exhibit suboptimal, often QMDP-like, behavior.
The experiments in \cref{sec:discgran} show this for POMCP-DPW and DESPOT, and this is presumably the case for ABT as well.


\begin{figure}[htpb]
    \centering
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{media/dpw_tree.pdf}
        \caption{POMCP-DPW Tree}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{media/pomcpow_tree.pdf}
        \caption{POMCPOW Tree}
    \end{subfigure}
    \caption[POMCP-DPW and POMCPOW tree structure comparison]{Tree structure comparison. Each square is an action node, and each unfilled circle is an observation node. Each black dot corresponds to a state particle with the size representing its weight. In continuous observation spaces, the beliefs in a POMCP-DPW tree degenerate to a single particle, while POMCPOW maintains weighted particle mixture beliefs.}
    \label{fig:treecomp}
\end{figure}

\subsection{PFT-DPW}

Another algorithm that one might consider for solving continuous POMDPs online is MCTS-DPW on the equivalent belief MDP.
Since the Bayesian belief update is usually computationally intractable, a particle filter is used.
This new approach will be referred to as particle filter trees with double progressive widening (PFT-DPW).
It is shown in \cref{alg:pft}, where $G_\text{PF($m$)}(b,a)$ is a particle filter belief update performed with a simulated observation and $m$ state particles which approximates the belief MDP generative model.
The authors are not aware of any mention of this algorithm in prior literature, but it is very likely that MCTS with particle filters has been used before without double progressive widening under another name.

PFT-DPW is fundamentally different from POMCP and POMCPOW because it relies on simulating approximate belief trajectories instead of state trajectories.
This distinction also allows it to be applied to problems where the reward is a function of the belief rather than the state such as pure information-gathering problems \cite{dressel2017efficient,araya2010pomdp}.

The primary shortcoming of this algorithm is that the number of particles in the filter, $m$, must be chosen a-priori and is static throughout the tree.
Each time a new belief node is created, an $\mathcal{O}(m)$ particle filter update is performed.
If $m$ is too small, the beliefs may miss important states, but if $m$ is too large, constructing the tree is expensive.
Fortunately, the experiments in \cref{sec:experiments} show that it is often easy to choose $m$ in practice; for all the problems studied here, a value of $m=20$ resulted in good performance.


\begin{algorithm}[htbp]
    \caption{PFT-DPW} \label{alg:pft}
    \begin{algorithmic}[1]
        \Procedure{Plan}{$b$}
            \For{$i \in 1:n$}
                \State $\Call{Simulate}{b, d_\text{max}}$
            \EndFor
            \State $\textbf{return } \underset{a}{\argmax}\, Q(ba)$
        \EndProcedure
        \Procedure {Simulate}{$b$, $d$}        
            \If{$d = 0$}
                \State \textbf{return} $0$
            \EndIf
            \State $a \gets \Call{ActionProgWiden}{b}$
            \If{$|C(ba)| \leq k_o N(ba)^{\alpha_o}$}
                \State $b',r \gets G_\text{PF($m$)}(b,a)$
                \State $C(ba) \gets C(ba) \cup \{(b',r)\}$
                \State $total \gets r + \gamma \Call{Rollout}{b', d-1}$
            \Else
                \State $b', r \gets \text{sample uniformly from } C(ba)$
                \State $total \gets r + \gamma \Call{Simulate}{b', d-1}$
            \EndIf
            \State $N(b) \gets N(b)+1$
            \State $N(ba) \gets N(ba)+1$
            \State $Q(ba) \gets Q(ba) + \frac{total - Q(ba)}{N(ba)}$
            \State \textbf{return} $total$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\subsection{POMCPOW} \label{sec:pomcpow}

\begin{algorithm}[htbp]
    \caption{POMCPOW} \label{alg:pomcpow}
    \begin{algorithmic}[1]
        \Procedure {Simulate}{$s$, $h$, $d$}        
            \If{$d = 0$}
                \State \textbf{return} $0$
            \EndIf
            \State $a \gets \Call{ActionProgWiden}{h}$
            \State $s',o,r \gets G(s,a)$
            \If{$|C(ha)| \leq k_o N(ha)^{\alpha_o}$}
                \State $M(hao) \gets M(hao) + 1$
            \Else
                \State $o \gets \text{select } o \in C(ha) \text{ w.p. } \frac{M(hao)}{\sum_{o} M(hao)}$
            \EndIf
            \State $\text{append } s' \text{ to } B(hao)$ \label{lin:insert}
            \State $\text{append } \odist(o \mid s, a, s') \text{ to } W(hao)$ \label{lin:weight}
            \If{$o \notin C(ha)$} \Comment{new node}
                \State $C(ha) \gets C(ha) \cup \{o\}$
                \State $total \gets r + \gamma \Call{Rollout}{s', hao, d-1}$
            \Else
                \State $s' \gets \text{select } B(hao)[i] \text{ w.p. } \frac{W(hao)[i]}{\sum_{j=1}^m W(hao)[j]}$ \label{lin:sample}
                \State $r \gets R(s,a,s')$
                \State $total \gets r + \gamma \Call{Simulate}{s', hao, d-1}$
            \EndIf
            \State $N(h) \gets N(h)+1$
            \State $N(ha) \gets N(ha)+1$
            \State $Q(ha) \gets Q(ha) + \frac{total - Q(ha)}{N(ha)}$
            \State \textbf{return} $total$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}


In order to address the suboptimality of POMCP-DPW, we now propose a new algorithm, POMCPOW, shown in \cref{alg:pomcpow}.
In this algorithm, the belief updates are weighted, but they also expand gradually as more simulations are added.
Furthermore, since the richness of the belief representation is related to the number of times the node is visited, beliefs that are more likely to be reached by the optimal policy have more particles.
At each step, the simulated state is inserted into the weighted particle collection that represents the belief (line~\ref{lin:insert}), and a new state is sampled from that belief (line~\ref{lin:sample}).
A simple illustration of the tree is shown in \Cref{fig:treecomp} to contrast with a POMCP-DPW tree.
Because the resampling in line~\ref{lin:sample} can be efficiently implemented with binary search, the computational complexity is $\mathcal{O}(n d \log(n))$.

\subsection{Discretization} \label{sec:discretization}

Discretization is perhaps the most straightforward way to deal with continuous observation spaces.
In this approach, the continuous observation space is simply divided into small discrete regions and solved as a discrete POMDP using conventional methods.
The results in \cref{tab:experiments} show that this approach is only sometimes effective.

\subsection{Observation Distribution Requirement}

It is important to note that, while POMCP, POMCP-DPW, and DESPOT only require a generative model of the problem,  both POMCPOW and PFT-DPW require a way to query the relative likelihood of different observations ($\odist$ in line~\ref{lin:weight}).
One may object that this will limit the application of POMCPOW to a small class of POMDPs, but we think it will be an effective tool in practice for two reasons.

First, this requirement is no more stringent than the requirement for a standard importance resampling particle filter, and such filters are used widely, at least in the field of robotics that the authors are most familiar with. 
Moreover, if the observation model is complex, an approximate model may be sufficient.

Second, given the implications of \cref{thm:qmdp}, it is difficult to imagine a tree-based decision-making algorithm or a robust belief updater that does not require some way of measuring whether a state belongs to a belief or history.
The observation model is a straightforward and standard way of specifying such a measure.
Finally, in practice, except for the simplest of problems, using POMCP or DESPOT to repeatedly observe and act in an environment already requires more than just a generative model.
For example, the authors of the original paper describing POMCP~\cite{silver2010pomcp} use heuristic particle reinvigoration in lieu of an observation model and importance sampling.

\section{Experiments} \label{sec:experiments}

Numerical simulation experiments were conducted to evaluate the performance of POMCPOW and PFT-DPW compared to other solvers. The open source code for the experiments is built on the POMDPs.jl framework \cite{egorov2017pomdps} and is hosted at \url{https://github.com/zsunberg/ContinuousPOMDPTreeSearchExperiments.jl}. In all experiments, the solvers were limited to \SI{1}{second} of computation time per step. Belief updates were accomplished with a particle filter independent of the planner, and no part of the tree was saved for re-use on subsequent steps. Hyperparameter values are shown in \cref{sec:hyper}.

\begin{sidewaystable}
    {
        \caption{Experimental Results} \label{tab:experiments}

        \input{results_table}
    }
    \vspace{5mm}
    \footnotesize{The three C or D characters after the solver indicate whether the state, action, and observation spaces are continuous or discrete, respectively. For continuous problems, solvers with a superscript D were run on a version of the problem with discretized action and observation spaces, but they interacted with continuous simulations of the problem.}
\end{sidewaystable}

\subsection{Laser Tag}

The Laser Tag benchmark (\cref{fig:lasertag}) is taken directly from the work of \citet{somani2013despot} and included for the sake of calibration. A complete description is given in that work. DESPOT outperforms the other methods in this test. The score for DESPOT differs slightly from that reported by \citet{somani2013despot} likely because of bounds implementation differences.
POMCP performs much better than reported by \citet{somani2013despot} because this implementation uses a state-based rollout policy.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.6\linewidth]{media/lasertag.png}
    \caption[Laser Tag benchmark]{Laser Tag benchmark. The green square is the robot; the orange square is the target. The varying shades of yellow show the belief about the position of the target. The red dashed lines represent the laser sensors.}
    \label{fig:lasertag}
\end{figure}

\subsection{Light Dark}

In the Light Dark domain, the state is an integer between $-60$ and $60$, and the agent can choose how to move deterministically ($s' = s+a$) from the action space $\mathcal{A}=\{-10, -1, 0, 1, 10\}$. 
The goal is to reach the origin and take an action there.
If action $0$ is taken at the origin, a reward of $100$ is given and the problem terminates; If action $0$ is taken at another location, a penalty of $-100$ is given.
There is a cost of $-1$ at each step before termination.
The agent receives a more accurate observation in the ``light'' region around $s=10$.
Specifically, observations are continuous ($\ospace=\reals$) and normally distributed with standard deviation $\sigma=|s-10|$.

\Cref{tab:experiments} shows the mean reward from $1000$ simulations for each solver, and \cref{fig:ld} shows an example experiment.
The optimal strategy involves moving toward the light region and localizing before proceeding to the origin. 
QMDP and solvers predicted to behave like QMDP attempt to move directly to the origin, while POMCPOW and PFT-DPW perform better.
In this one-dimensional case, discretization allows POMCP to outperform all other methods and DESPOT to perform well, but in subsequent problems where the observation space has more dimensions, discretization does not provide the same performance improvement (see \cref{sec:discretization}).

\begin{figure}[htb]
    \centering
    % \input{ld_fig.pgf}
    \includegraphics[width=0.6\columnwidth]{media/ld_fig.pdf}
    \caption[Light Dark problem]{Example trajectories in the Light Dark domain. POMCPOW travels to the light region and accurately localizes before moving to the goal. POMCP-DPW displays QMDP-like behavior: it is unable to localize well enough to take action \num{0} with confidence. The belief particles far away from \num{0} in the POMCP-DPW plot are due to particle reinvigoration that makes the filter more robust.}
    \label{fig:ld}
\end{figure}

\subsection{Sub Hunt}

In the Sub Hunt domain, the agent is a submarine attempting to track and destroy an enemy sub.
The state and action spaces are discrete so that QMDP can be used to solve the problem for comparison.
The agent and the target each occupy a cell of a 20 by 20 grid. The target is either aware or unaware of the agent and seeks to reach a particular edge of the grid unknown to the agent ($\mathcal{S} = \{1,..,20\}^4 \times \{\text{aware}, \text{unaware}\} \times \{N,S,E,W\}$).
The target stochastically moves either two steps towards the goal or one step forward and one to the side.
The agent has six actions, move three steps north, south, east, or west, engage the other submarine, or ping with active sonar.
If the agent chooses to engage and the target is unaware and within a range of 2, a hit with reward 100 is scored; The problem ends when a hit is scored or the target reaches its goal edge.

An observation consists of 8 sonar returns ($\ospace = \reals^8$) at equally-spaced angles that give a normally distributed estimate ($\sigma=0.5$) of the range to the target if the target is within that beam and a measurement with higher variance if it is not.
The range of the sensors depends on whether the agent decides to use active sonar.
If the agent does not use active sonar it can only detect the other submarine within a radius of 3, but pinging with active sonar will detect at any range.
However, active sonar alerts the target to the presence of the agent, and when the target is aware, the hit probability when engaging drops to $60\%$.

\Cref{tab:experiments} shows the mean reward for $1000$ simulations for each solver.
The optimal strategy includes using the active sonar, but previous approaches have difficulty determining this because of the reduced engagement success rate.
The PFT-DPW approach has the best score, followed closely by POMCPOW.
All other solvers have similar performance to QMDP.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.6\linewidth]{media/subhunt.png}
    \caption[Sub Hunt problem]{Sub Hunt benchmark. The color of each location square shows the number of belief particles containing the target at that location. In this case, the sub has incorrectly concluded that, with high probability, the target is moving towards the east and is chasing that belief concentration.}
    \label{fig:subhunt}
\end{figure}


\subsection{Van Der Pol Tag} \label{sec:vdptag}

\begin{figure}[bth]
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[height=2in]{media/vdp_quiver.pdf}
        \caption{Van Der Pol Dynamics. The arrows show the target differential equation, and the thick black lines represent the barriers.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[height=2in]{media/vdptag.png}
        \caption{Van Der Pol Tag. The agent attempts to tag the target, but cannot pass through barriers.}
    \end{subfigure}
    \caption[Van Der Pol tag problem]{Van Der Pol tag problem}
    \label{fig:vdp}
\end{figure}


The final experimental problem is called Van Der Pol tag and has continuous state, action, and observation spaces.
In this problem an agent moves through 2D space to try to tag a target ($\mathcal{S}=\reals^4$) that has a random unknown initial position in $[-4, 4]\times[-4,4]$.
The agent always travels at the same speed, but chooses a direction of travel and whether to take an accurate observation ($\mathcal{A} = [0, 2\pi)\times\{0,1\}$).
The observation again consists of 8 beams ($\ospace=\reals^8$) that give measurements to the target.
Normally, these measurements are too noisy to be useful ($\sigma=5$), but, if the agent chooses an accurate measurement with a cost of \num{5}, the observation has low noise ($\sigma=0.1$).
The agent is blocked if it comes into contact with one of the barriers that stretch from \num{0.2} to \num{3.0} in each of the cardinal directions (see \cref{fig:vdp}), while the target can move freely through.
There is a cost of \num{1} for each step, and a reward of \num{100} for tagging the target (being within a distance of \num{0.1}).


The target moves following a two dimensional form of the Van Der Pol oscillation defined by the differential equations%
\begin{equation}
    \dot{x} = \mu \left( x - \frac{x^3}{3} -y \right) \quad \text{ and }\quad \dot{y} = \frac{1}{\mu}x\text{,} \nonumber
\end{equation}
where $\mu=2$.
Gaussian noise ($\sigma=0.05$) is added to the position at the end of each step.
Runge-Kutta fourth order integration is used to propagate the state.

This problem has several challenging features that might be faced in real-world applications.
First, the state transitions are more computationally expensive because of the numerical integration.
Second, the continuous state space and obstacles make it difficult to construct a good heuristic rollout policy, so random rollouts are used.
\Cref{tab:experiments} shows the mean reward for $1000$ simulations of this problem for each solver.
Since a POMCPOW iteration requires less computation than a PFT-DPW iteration, POMCPOW simulates more random rollouts and thus performs slightly better.

\subsection{Multilane}

The final problem for evaluation is the multiple-lane-change problem described in \cref{sec:multilanepomdp}.
Since costly information gathering is not an important part of this problem, POMCP-DPW and POMCPOW have similar performance.
Because it uses a fixed number of scenarios and bounds to control exploration rather than progressive widening and a UCB heuristic, DESPOT is able to make better long term plans and is the most effective in this problem.
It is also important to note that in this problem, the most realistic of the test problems, PFT-DPW performs significantly worse than POMCPOW.

\subsection{Discretization granularity} \label{sec:discgran}

\Cref{fig:disc} shows the performance at different discretization granularities for the Light Dark and Sub Hunt problems.

Since the Light Dark domain has only a single observation dimension, it is easy to discretize.
In fact, POMCP with fine discretization outperforms POMCPOW.
However, discretization is only effective at certain granularities, and this is highly dependent on the solver and possibly hyperparameters.
In the Sub Hunt problem, with its high-dimensional observation, discretization is not effective at any granularity.
In Van Der Pol tag, both the action and observation spaces must be discretized.
Due to the high dimensionality of the observation space, similar to Sub Hunt, no discretization that resulted in good performance was found.

\begin{figure}[htb]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=0.97\linewidth]{media/ld_discretization.pdf}
        \caption{Light Dark} \label{fig:lddisc}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{media/subhunt_discretization.pdf}
        \caption{Sub Hunt}
        \label{fig:shdisc}
    \end{subfigure}

    \caption{Discretization granularity studies} \label{fig:disc}
\end{figure}


\subsection{Hyperparameters} \label{sec:hyper}

\begin{table}[htbp]
    {\centering
\caption{Hyperparameters used in experiments} \label{tab:hyper}

\begin{center}
\begin{tabular}{lrrrrr}
    \toprule
    POMCPOW     & Laser Tag     & Light Dark    & Sub Hunt      & VDP Tag     & Multilane \\
    \midrule
    $c$         & \num{26.0}    & \num{90.0}    & \num{17.0}    & \num{110.0} & \num{8.0} \\
    $k_a$       & --            & --            & --            & \num{30.0}  & -- \\
    $\alpha_a$  & --            & --            & --            & \num{1/30}  & -- \\
    $k_o$       & \num{4.0}     & \num{5.0}     & \num{6.0}     & \num{5.0}   & \num{4.5} \\
    $\alpha_o$  & \num{1/35}    & \num{1/15}    & \num{1/100}   & \num{1/100} & \num{1/10} \\
    \midrule
    PFT-DPW     & Laser Tag     & Light Dark    & Sub Hunt      & VDP Tag     & Multilane \\
    \midrule
    $m$         & \num{20}      & \num{20}      & \num{20}      & \num{20}    & \num{15} \\
    $c$         & \num{26.0}    & \num{100.0}   & \num{100.0}   & \num{70.0}  & \num{8.0} \\
    $k_a$       & --            & --            & --            & \num{20.0}  & -- \\
    $\alpha_a$  & --            & --            & --            & \num{1/25}  & -- \\
    $k_o$       & \num{4.0}     & \num{4.0}     & \num{2.0}     & \num{8.0}   & \num{4.5} \\
    $\alpha_o$  & \num{1/35}    & \num{1/10}    & \num{1/10}    & \num{1/85}  & \num{1/10} \\
    \bottomrule

\end{tabular}

    \vspace{1mm}
    \footnotesize{For problems with discrete actions, all actions are considered and $k_a$ and $\alpha_a$ are not needed.}
\end{center}
    }
\end{table}

Hyperparameters for POMCPOW and PFT-DPW were chosen using the cross entropy method \cite{mannor2003cross}, but exact tuning was not a high priority and some parameters were re-used across solvers so the parameters may not be perfectly optimized.
The values used in the experiments are shown in \cref{tab:hyper}. 
There are not enough experiments to draw broad conclusions about the hyperparameters, but it appears that performance is most sensitive to the exploration constant, $c$.

The values for the observation widening parameters, $k_o$ and $\alpha_o$, were similar for all the problems in this work.
A small $\alpha_o$ essentially limits the number of observations to a static number $k_o$, resulting in behavior reminiscent of sparse UCT \cite{browne2012survey}, preventing unnecessary widening and allowing the tree to grow deep.
This seems to work well in practice with the branching factor ($k_o$) set to values between \num{2} and \num{8}, and suggests that it may be sufficient to limit the number of children to a fixed number rather than do progressive widening in a real implementation.

\section{Summary}

The results in this chapter clearly show that the previous leading online solvers are unable to adequately solve POMDPs with continuous observation spaces where exploration is important.
In particular, \cref{thm:qmdp} explains why these methods behave similarly to QMDP.
POMCPOW and PFT-DPW overcome this limitation and outperform QMDP.
Though POMCPOW only performs strictly best on one of the problems in \cref{sec:experiments}, it performs very well on all of them, so it might be considered the best overall algorithm in these tests.

However there are still several shortcomings.
First, I have not proven analytically that POMCPOW or PFT-DPW converges toward the optimal solution.
% , and, given the complexity of the proof for DPW in the fully-observable case \cite{auger2013continuous}, this seems to be a relatively challenging task.
Second, double progressive widening is made more difficult in practice because of its tuning parameters.
Third, the results from the Multilane experiment shows that DESPOT is able to find significantly better plans in more realistic problems.
Thus, much work remains along this trajectory.
DESPOT has several attractive attributes compared to the new algorithms presented here.
In particular, its use of a fixed number of scenarios to control widening, and bounds to control exploration may make it more amenable to analysis and allow it to perform better in practice.
Extending DESPOT to handle continuous observation spaces seems to be a promising future direction.
