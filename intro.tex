\chapter{Introduction}

Safety and efficiency are two important goals for technology in many domains.
This thesis focuses on the autonomous vehicle domain, an application area that is currently growing quickly.
Before outlining specific research results, this chapter gives an introduction to some benefits of and challenges for autonomous vehicles and some of the basic theoretical structures for approaching the problem of controlling them.

\section{Autonomous Vehicles}

% Transportation is one of the most important ingredients to human civilization.
Until now, vehicles have been almost exclusively operated by humans.
However, recent advances in sensing technology, computing power, mapping, data processing, and connectivity have made it possible to begin developing vehicles that are \emph{autonomous}, that is, they can perform a transportation task without requiring a human to control them.

\subsection{Benefits of Autonomous Transportation} \label{sec:benefits}

Autonomous vehicles have the potential to bring a wide range of benefits.
The first of these benefits is \textbf{improved safety}.
Around the world, more than 1.2 million people are killed every year by vehicles~\cite{who2015global}, and, according to a study concluded in 2007, the critical reason for more than \SI{90}{\percent} of motor vehicle crashes in the United States is driver error~\cite{nhtsa2015critical}.
Since they are not affected by many of the crash-causing deficiencies that human drivers have, including distraction, tiredness, and impairment due to drugs or alcohol, autonomous vehicles have the potential to greatly reduce the number of injuries and deaths.

Another potential benefit of autonomous vehicles is \textbf{increased collective efficiency}.
Currently, middle- and low-income nations lose up to \SI{3}{\percent} of GDP due to road vehicle accidents, many of which could be mitigated with autonomy~\cite{who2015global}.
Moreover, autonomy has the potential to enable new models of vehicle usage for cities.
For example, a city-wide mobility-on-demand (MOD) system consisting of purpose-built electric autonomous vehicles could provide a much more efficient system for commuting than the current fleet of individually-owned gas-powered vehicles optimized for factors other than commuting efficiency.
The barriers to adopting such a system may be greatly reduced by autonomous vehicles' ability to, for instance, automatically travel to a charging station or rebalance to places where additional vehicles are needed.
A recent study concluded that Manhattan taxi demand could be met with a fleet of autonomous cars \SI{60}{\percent} the size of the current taxi fleet \cite{RZ-MP:15_MODa}.

The third benefit is the potential for \textbf{reduced personal transportation costs} in terms of time and stress.
In the United States, workers commute an average of more than \SI{25}{\minute} in each direction~\cite{census2016travel}, and more than three quarters drive alone~\cite{mckenzie2015who}.
Thus a large fraction of American workers use an hour of their day focused on operating a vehicle.
With autonomous vehicles, this stressful time could be eliminated and replaced with productive time or rest.

The final potential benefit that will be listed here is \textbf{expanding access to mobility}.
The requirement for a capable vehicle operator prevents many people from using cars.
For example, many people with permanent disabilities such as blindness or seizure disorders or even temporary injuries are precluded from the flexible personal point-to-point transportation afforded by cars, and this increases their dependence on caretakers.
Autonomous cars would give these people safe and flexible personal transportation.

\subsection{Current Progress}

There has been a great deal of progress towards autonomy.
For example, several companies including Amazon~\cite{shaban2018amazon} and Alphabet~\cite{sandoval2018alphabet}, are pursuing delivery systems consisting of autonomous aerial drones.
A focal point of early ground vehicle development was the 2007 DARPA Urban Challenge~\cite{MB-KI-SS:10}, which was the first major demonstration of self-driving cars operating in a simulated urban environment.
Since that time, significant progress has been made and testing operations in a small number of urban environments has become routine~\cite{dolgov2016google}.

\subsection{Remaining Challenges}

Despite the considerable progress made towards autonomy, there are currently no large-scale commercial applications of autonomous vehicles, and several challenges remain.
This section does not give an exhaustive list of the challenges remaining for deployment of autonomous vehicles, but it outlines a few and discusses how this thesis relates to them.

\subsubsection{Technical}

Some of the largest remaining technical challenges for autonomous vehicle development are challenges of generalization.
Many of the components for navigating within a closed test space had already been developed by the time of the DARPA urban challenge in 2007~\cite{MB-KI-SS:10}.
An industry leader has noted that ``it's relatively easy to master the first \SI{90}{\percent} of driving where you're traveling on freeways, navigating light city-street traffic, or making your way through simple intersections''~\cite{dolgov2016google}.
However, autonomous vehicle testing is still limited to relatively small well-mapped environments because it is difficult to generalize solutions to new domains.

In order to handle a larger and more diverse set of circumstances, autonomous vehicle designers must both gather more data~\cite{dolgov2016google} and design control systems to operate in the new environment.
One approach to control system design is to iterate with a two step process consisting of (1) specifying a control policy by hand, and (2) evaluating these actions against some criteria.
Examples of this approach are hand-tuned linear control systems and the specified behaviors of some of the urban challenge competitors (e.g.~\citet{urmson2007tartan}).
A second approach is to use algorithms that systematically and automatically determine a good control policy according to some criterion.
While the first approach is adequate for some simple constrained domains, the second is more likely to work in regimes that the designers have not explicitly tested.
Moreover, the second approach reduces the number tasks required to adapt to a new operational environment from two (identifying a model and designing a control system) to one (identifying a model).
The algorithms and applications in this thesis belong to the latter family and improve our ability to apply the approach to problems with complex and uncertain dynamics.

There are many other technical challenges including perception in adverse conditions, testing, security, cost reduction, linking perception with reasoning, and others, but they are not directly addressed in this work.

\subsubsection{Ethical, Legal, and Economic}

In addition to the technical challenges, there are a wide range of ethical, legal, and economic challenges, including liability assignment, job displacement, and discrimination against certain populations, that have the potential to hinder the success of autonomous vehicles~\cite{casey2016amoral}.
This thesis does not seek to systematically address moral or legal challenges, but it does speak to one in particular.
Autonomous vehicles must often make decisions that balance the interests of multiple people.
For instance, in addition to the performance criteria that the vehicle operator desires, there may be \emph{externalities} that affect the well-being of other people.
There is often a tradeoff between these objectives.
For instance, traveling with increased speed will allow the owner to transport passengers or cargo faster, but it may endanger bystanders.
The results in this thesis show that, for the specific competing goals of safety and efficiency, both can be simultaneously improved by using better models and algorithms (see \cref{chap:uav,chap:multilane}).
It is likely that this principle will also apply to other tradeoffs.
Better algorithmic and modeling approaches may be an answer in cases where competing ethical, legal, and economic principles are at odds.

% \subsubsection{Economic}
% 
% There are also questions surrounding the economics of autonomous vehicles including what model of ownership is most efficient, whether 

\section{Decision Making Under Uncertainty}

The task of controlling an autonomous vehicle is a task of decision making under uncertainty.
That is, the vehicle control system must decide what actions to take to achieve good performance, even when there is uncertainty about the world around it and what will happen in the future.
There are two ingredients needed to formulate such a problem.
The first is a criterion for good performance, and the second is a model of what is known about the world and how it will evolve.

\subsection{Optimization objective}

This thesis casts decision making problems as optimization problems.
That is, the criterion for good performance is that the actions chosen maximize an objective function,\footnote{This thesis makes no distinction between objective \emph{functional}s that map from spaces of functions to scalar values and objective functions that map from scalar or vector spaces to scalar values. The term ``objective function'' will be used for both.}
\begin{equation}
\underset{\pi}{\text{maximize}} \quad J(\pi)
\end{equation}
where $\pi$ determines what actions are taken depending on the inputs. The concrete structures for $\pi$ and $J$ are presented in \cref{sec:mdps}, but a brief introduction is given here.

To define an objective function, an engineer must map the desires of human passengers or abstract societal goals like those described in \cref{sec:benefits} into a well-defined mathematical function.
This is sometimes straightforward for individual goals such as minimizing the time to reach a destination.
However, combining multiple goals is often a challenge because an optimization problem is not, in general, well-posed if it has multiple objective functions.

One solution to this problem is to optimize a single objective, $J_0$, and constrain all other objectives, $J_i$, to meet specified thresholds, $D_i$~\cite{EA:99},
\begin{equation}
    \begin{aligned}
    & \underset{\pi}{\text{maximize}} & & J_0(\pi) \\
    & \text{subject to} & & J_1(\pi) \geq D_1 \\
    &                   & & J_2(\pi) \geq D_2 \\
    &                   & & \ldots \\
    &                   & & J_n(\pi) \geq D_n \text{.}
    \end{aligned}
\end{equation}
However many off-the-shelf decision-making algorithms must be modified to handle this formulation, and choosing the thresholds adds another step to the development process.

Another alternative is to create a single objective function that is a weighted sum of the functions corresponding to the individual rewards,
\begin{equation}
    \underset{\pi}{\text{maximize}} \quad J_0(\pi) + \lambda_1 J_1(\pi) + \lambda_2 J_2(\pi) + \ldots + \lambda_n J_n(\pi) \text{,}
\end{equation}
where $\lambda_i$ are the relative weights.
This scalarization approach allows conventional decision-making algorithms to be used, but choosing appropriate weights before solving the problem is often difficult.

Another challenge is that human preferences can be difficult to quantify and encode into a reward function.
For these cases, inverse reinforcement learning~\cite{levine2012continuous, sadigh2016leverage} or preference elicitation~\cite{lepird2015bayesian} can be used to determine a suitable reward function based on data recording how humans act.

This thesis does not make significant contributions to understanding how to choose appropriate objectives, but instead adopts the weighted sum scalarization approach to combine a few important objectives.
In particular, the investigation focuses on the goals of safety and efficiency, which are balanced using a scaling factor, $\lambda$,
\begin{equation} \label{eq:scalarized}
    \underset{\pi}{\text{maximize}} \quad J_E(\pi) + \lambda \, J_S(\pi) \text{.}
\end{equation}
It adopts a stance that is agnostic to the value $\lambda$ by focusing on Pareto frontier analysis.

A solution is said to be \emph{Pareto-optimal} if no objective can be improved without adversely affecting another objective.
In particular, every solution to problem (\ref{eq:scalarized}) for a positive value of $\lambda$ is Pareto-optimal, though there may also be additional Pareto-optimal points that do not correspond to solutions of this problem.
The \emph{Pareto front} is the set of all Pareto-optimal solutions.

Since exact solutions to the problems studied in this thesis are intractable, it is impossible to reliably generate points on the true Pareto front.
Instead, approximate Pareto fronts are constructed by connecting approximate solutions to (\ref{eq:scalarized}) with straight lines (see \cref{fig:pareto} for an example), and conclusions about different algorithms can be reached by comparing the resulting curves generated by different algorithms.
Given two algorithms, A and B, if the Pareto front for A is in a better position than that of B for most or all values of $\lambda$, then it is possible to argue that algorithm A is superior to algorithm B without committing to a particular value of $\lambda$.

\subsection{Uncertainty in Decision Making}

The second primary ingredient to a decision making problem is a model of the randomness in the mapping from actions to the objective function.
This thesis is concerned with three types of uncertainty: outcome, model, and state.
Although the definitions of these types are not mathematically formal, they are useful for conceptual understanding.
The distinctions between the types are defined based on the concept of ``Markov state''.

\subsubsection{Markov state}

Consider the case where the configuration of the world changes over time according to a dynamics model.
This thesis only considers models where these changes occur at specific discrete time intervals.
A \emph{Markov state}, or simply \emph{state}, consists of the world configuration and possibly some other information specially chosen so that the state at the next time step depends only on the current state and not on any previous states.
This property of independence from past history is known as the \emph{Markov property}.
In general, the state may be a random variable, and, in this case, the next state must be stochastically independent of all previous states when conditioned on the current state.

\subsubsection{Outcome Uncertainty}

The simplest type of uncertainty is outcome uncertainty.
Outcome uncertainty is uncertainty in the future state even when the current state and model are known exactly.
In other words, it refers to the inherent stochasticity in the model.
The distinguishing property of outcome uncertainty is that nothing can be learned by the agent to reduce it.

\subsubsection{Model Uncertainty}

A second category for uncertainty is model uncertainty.
In some cases, the dynamic model of the system is not known to the agent, and this can be a source of uncertainty in the future of the state that compounds the outcome uncertainty.
The distinction is that, over time the agent can observe how the state changes to learn more about the model and reduce this type of uncertainty.
Methods that attempt to optimize an objective in the face of this type of uncertainty are usually referred to as \emph{reinforcement learning} methods \cite{RSS-AGB:98}.

\subsubsection{State Uncertainty}

A third type of uncertainty is state uncertainty.
When there is state uncertainty, the agent cannot directly observe the state, and instead receives only observations that offer clues about it.
However, it may reason about what the state could be and make better decisions based on this partial knowledge.
The distinction between state and model uncertainty is that the state may change over time, while the model does not.
Model uncertainty can be represented as state uncertainty by augmenting the state to include the unknown model parameters.

\subsection{Markov Decision Processes} \label{sec:mdps}

The Markov decision process (MDP) is a mathematical formalism that can represent a wide range of sequential decision making problems~\cite{DB:05,kochenderfer2015decision}. 
In an MDP, an agent takes \emph{actions} that affect the state of the system and collects \emph{rewards} based on the states and actions.
The MDP formalism is able to represent outcome uncertainty with a stochastic state transition model.
Typically the state at time $t$ will be denoted $s_t$, and the action at time $t$ denoted $a_t$, however in some contexts where only one step of time is relevant, the subscripts will be dropped for the sake of clarity and brevity, and $s'$ will be used to denote the next state that the system transitions into.

Formally, an MDP is defined by the 5-tuple $(\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma)$.
The state space, $\mathcal{S}$, is the set of all possible states.
The action space, $\mathcal{A}$, is the set of all actions available to the agent.
The transition model, $\mathcal{T}$, represents the likelihood of different transitions, where $\mathcal{T}(s' \mid s, a)$ denotes the probability that the system will transition to state $s'$ given that action $a$ is taken in state $s$.
The reward function, $\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \reals$ represents the rewards received while interacting in the environment, where
$\mathcal{R}(s, a, s')$ denotes the reward for transitioning from $s$ to $s'$ when action $a$ is taken.
Finally, $\gamma$ governs how reward is discounted in the future.

The objective in an MDP is to find a policy, $\pi: \mathcal{S} \to \mathcal{A}$, that maps each encountered state to an action and, when $a_t = \pi(s_t)$, maximizes the objective function.
Because of the outcome uncertainty, the rewards that a policy will achieve are random variables.
Thus, for the optimization problem to be well-posed, the objective must be a function of the distribution of the rewards.
For a Markov decision process, the objective is to maximize the cumulative expected reward,\footnote{Sequential decision problems where the objective involves maximizing a measure other than the expectation of the reward have been studied, but are much less common, and their solution is sometimes much more difficult than in the expectation case~\cite{chow2015risk}.}
\begin{equation} \label{eq:cumulative}
    J(\pi) = E\left[\sum_{t=0}^\infty \gamma^t \mathcal{R}(s_t, \pi(s_t), s_{t+1})\right] \text{.}
\end{equation}

It is important to note that it is sufficient to consider only the class of deterministic Markov policies, that is, policies that map each state to a single action, when searching for an optimal solution.
It has been proven that, for a policy of any structure, e.g. a policy that is stochastic or a policy that depends on more than just the state, there exists a Markov policy that can attain the same objective value.
Consequently, it is always possible to find a deterministic Markov policy that achieves the optimal objective value \cite{EA:99}.

The objective of an MDP is sometimes discussed in terms of the optimal value function $V^*: \sspace \to \reals$, which is the cumulative expected reward given that the system starts in a specified state.
The optimal state action value function, $Q^*(s,a)$, is defined as the expectation of the future cumulative reward given that the agent starts in state $s$, immediately takes action $a$, and then follows the optimal policy~\cite{DB:05,kochenderfer2015decision,EA:99}.

\subsection{Partially Observable Markov Decision Processes} \label{sec:pomdps}

A partially observable Markov decision process (POMDP) is similar to an MDP except that the agent cannot directly observe the state.
Instead, the agent only has access to observations that are generated probabilistically based on the actions and latent true states.
In this way, a POMDP is able to represent state uncertainty in addition to the outcome uncertainty that can be encoded in an MDP.
A POMDP is defined by the 7-tuple $(\sspace, \aspace, \tdist, \reward, \ospace, \odist, \gamma)$, where $\sspace$, $\aspace$, $\tdist$, $\reward$, and $\gamma$ have the same meaning as in an MDP.
Additionally, $\ospace$, is the observation space, and $\odist$ is the observation model.
$\odist(o \mid s, a, s')$ is the probability or probability density of receiving observation $o$ in state $s'$ given that the previous state and action were $s$ and $a$~\cite{DB:05,kochenderfer2015decision,kaelbling1998planning}.

Information about the state may be inferred from the entire history of previous actions and observations and the initial information, $b_0$.
Thus, in a POMDP, the agent's policy is a function mapping each possible history, $h_t = (b_0, a_0, o_1, a_1, o_2, \dots, a_{t-1}, o_t)$ to an action.
In some cases, each state's probability can be calculated based on the history.
This distribution is known as a \emph{belief}, with $b_t(s)$ denoting the probability of state $s$.

The belief is a sufficient statistic for optimal decision making.
That is, there exists a policy, $\pi^*$ such that, when $a_t = \pi^*(b_t)$, the expected cumulative reward or ``value function'' is maximized for the POMDP \cite{kaelbling1998planning,kochenderfer2015decision}.
Given the POMDP model, each subsequent belief can be calculated using Bayes' rule according to
\begin{equation} \label{eqn:update}
    b'(s') = \frac{\int_{s\in\mathcal{S}} \odist(o \mid s, a, s') \mathcal{T}(s' \mid s, a) b(s) \, ds}
    {\int_{s'\in\mathcal{S}} \int_{s\in\mathcal{S}} \odist(o \mid s, a, s') \mathcal{T}(s' \mid s, a) b(s) \, ds \, ds'} \text{.}
\end{equation}
When the state space is discrete, the integrals may be replaced with sums.

Unfortunately, it has been shown that even the class of finite-horizon POMDPs is PSPACE-complete, indicating that it is extremely unlikely that efficient general exact algorithms for large problems will be discovered \cite{papadimitriou1987complexity}.
Because of this, approximate algorithms are often used.

\subsubsection{Generative Models}

For many problems, it can be difficult to explicitly determine or represent the probability distributions $\tdist$ or $\odist$.
Some solution approaches, however, only require samples from the state transitions and observations.
A generative model, $G$, stochastically generates a new state, reward, and observation in the partially observable case, given the current state and action, that is $s', r = G(s,a)$ for an MDP, or $s', o, r = G(s, a)$ for a POMDP.
A generative model implicitly defines $\tdist$ and $\odist$, even when they cannot be explicitly represented.

\subsubsection{Belief MDP}

Every POMDP is equivalent to an MDP where the state space of the MDP is the space of possible beliefs.
The reward function of this "belief MDP" is the expectation of the state-action reward function with respect to the belief.
The Bayesian update of the belief serves as a generative model for the belief space MDP.

\subsection{Value Iteration}

Value iteration is the simplest offline method for solving MDPs.
It relies on the Bellman equation, which characterizes the optimal value function,
\begin{equation}
    V^*(s) = \max_{a \in \aspace} \left\{\reward(s,a) + \text{E}\left[V^*(s') \mid s, a \right]\right\} \text{.}
\end{equation}
For discrete problems, the unique solution to the Bellman equation, $V^*$, can be found through repeated application of the Bellman operator,
\begin{equation}
    \mathcal{B}[V](s) = \max_{a \in \aspace} \left\{\reward(s,a) + \text{E} \left[V(s') \mid s, a \right] \right\} \text{.}
\end{equation}
When $\mathcal{B}$ is applied iteratively, i.e.\ $V_{k+1} = \mathcal{B}[V_k]$, the value function will eventually reach  a fixed point when $V_{k+1} = V_k$.\footnote{The uniqueness of the fixed point and convergence to that fixed point are guaranteed because the Bellman operator is a contraction mapping~\cite{johnsonbaugh2012foundations}.} This point is the optimal value function, and the process of finding the value function in this manner is known as value iteration~\cite{DB:05}.

Once the optimal value function has been found, the optimal policy may be easily extracted according to 
\begin{equation}
    \pi^*(s) = \argmax_{a \in \aspace} \left\{\reward(s,a) + \text{E}\left[V^*(s') \mid s, a \right]\right\} \text{.}
\end{equation}

\subsection{Monte Carlo Tree Search} \label{sec:mcts}

In many cases, the state and action spaces of a problem are so large that computing a complete solution offline is intractable.
Instead, approximate solutions valid only for the current state may be calculated online during execution in the environment.
One way of accomplishing this is to create a policy tree consisting of alternating layers of state nodes and action nodes that evaluates every possible future trajectory.
Unfortunately, it is typically infeasible to construct a sufficiently deep tree because construction is exponentially complex with respect to the depth~\cite{kochenderfer2015decision}.

Monte Carlo tree search (MCTS) is a method that uses Monte Carlo simulations to incrementally construct only important parts of the policy tree.
By estimating the state-action value function, $Q(s,a)$, for each of the action nodes within the tree and focusing further exploration on nodes with high values, it can achieve good results without attempting the exponentially complex task of constructing the entire policy tree.
MCTS is widely studied and has proven to be effective in a variety of contexts~\cite{browne2012survey}.
Only a generative model, $G$, is required by the algorithm.
Although there are many variations of this algorithm, they are built around a common Monte Carlo simulation process that is repeated $n$ times.
Each simulation consists of the following four steps:
\begin{enumerate}
    \item \emph{Search}. In the initial phase of the simulation, the policy defined by the tree is used.
        At each state node, a selection criterion based on $Q$ is used to choose a favorable action, and the tree is traversed through the node and to the next state node determined by $G$.
    \item \emph{Expansion}. Eventually, the simulation reaches an action node that does not have any children.
        At this point, a new state is sampled with $G$ and a corresponding node created along with children corresponding to each action. 
    \item \emph{Rollout}. After the expansion step, the simulation is continued with a rollout policy, often consisting of randomly selected actions, until the future accumulated reward will be negligible because of the compounding discount factor.
    \item \emph{Update}. Once the simulation has terminated, the estimates of $Q(s,a)$ at each of the visited action nodes are updated with the discounted reward received during the simulation after visiting the node.
\end{enumerate}
This approach builds a tree asymmetrically favoring regions of the state and action spaces that are likely to be visited when the optimal policy is executed.

\subsubsection{Upper Confidence Trees} \label{sec:uct}

The selection criterion used to choose actions in the search phase is very important.
It must balance favoring actions with large $Q$ values that are expected to yield good results with exploration of new actions.
A widely used approach for this is the upper confidence bound for trees (UCT) algorithm~\cite{browne2012survey}.
At each state node, it chooses the action that maximizes the upper confidence bound
\begin{equation} \label{eqn:ucb}
    UCB(s,a) = Q(s,a) + c \sqrt{\frac{\log N(s)}{N(s,a)}}
\end{equation}
where $N(s,a)$ is the number of times the action node has been visited, $N(s) = \sum_{a \in \mathcal{A}} N(s,a)$, and $c$ is a problem-specific parameter that governs the amount of exploration in the tree.
The second term causes the algorithm to favor actions that have been taken less often.

\subsubsection{Double Progressive Widening} \label{sec:dpw}

In cases where the action and state spaces are large or continuous, the MCTS algorithm will produce trees that are very shallow.
In fact, if the action space is continuous, the UCT algorithm will never try the same action twice (it is understood that, if $N(s,a) = 0$ then $UCB(s,a)$ in (\ref{eqn:ucb}) is infinite, so untried actions are always favored).
Moreover, if the state space is continuous and the transition probability density is finite, the probability of sampling the same state twice from $G$ is zero.
Because of this, simulations will never pass through the same state node twice and a tree below the first layer of state nodes will never be constructed.

In progressive widening, the number of children of a node is artificially limited to $k N^\alpha$, where $N$ is the number of times the node has been visited and $k$ and $\alpha$ are parameters chosen for the problem \cite{couetoux2011double}.
Originally, progressive widening was applied to the action space, and was found to be especially effective when a set of preferred actions was tried first \cite{browne2012survey}.
The term \emph{double} progressive widening refers to progressive widening in both the state and action space.
When the number of state nodes is greater than the limit, instead of simulating a new state transition, one of the previously generated states is chosen with probability proportional to the number of times it has been previously generated.


\subsection{Particle Filtering} \label{sec:particle}

Aside from a few special cases, for example when the system dynamics are linear and the transition and observation distributions are Gaussian, the integrals in the Bayesian belief update (\ref{eqn:update}) are impossible or difficult to solve analytically.
Thus, numerical approaches must be used.
A popular technique for this is particle filtering, usually incorporating domain-specific heuristic modifications to prevent problems such as particle depletion \cite{thrun2005probabilistic}.

Sequential importance resampling, one of the most common and effective variations, requires a state generative model, $G_s$ that can sample from the transition distribution and an explicit representation of the observation distribution, $\odist(\cdot \mid s, a, s')$.
The belief is approximated with a set of $m$ particles, $\{s_i\}_{i=1}^m$ and associated weights, $\{w_i\}_{i=1}^m$.
The probability of each state is approximated by the sum of the weights corresponding to particles with that state value,
\begin{equation}
    b(s) \approx \sum_{i=1}^m w_i \delta_s (s_i)
\end{equation}
where $\delta_s(\cdot)$ is the Dirac delta function centered at $s$.
A belief update is approximated by sampling $m$ states $\{\tilde{s}_i\}_{i=1}^m$ from the collection of particles with probability proportional to the associated weight, generating a particle, $\tilde{s}'_i = G(\tilde{s}_i, a)$ for each of these states, and finally setting the new weight proportional to the probability of generating the measured observation with this state, $w'_i \propto \odist(o \mid \tilde{s}_i, a, \tilde{s}'_i)$.

\subsection{Approximate Solutions to POMDPs} \label{sec:solutions}

Considerable progress has been made in solving large POMDPs.
Initially, exact offline solutions to problems with only a few discrete states, actions, and observations were sought by using value iteration and taking advantage of the convexity of the value function \cite{kaelbling1998planning}, although solutions to larger problems were also explored using Monte Carlo simulation and interpolation between belief states \cite{thrun1999monte}.
Many effective offline planners for discrete problems use point based value iteration, where a selection of points in the belief space are used for value function approximation \cite{kurniawati2008sarsop}.
Offline solutions for problems with continuous state and observation spaces have also been proposed \cite{bai2014integrated,brechtel2013solving}.

There are also various solution approaches that are applicable to specific classes of POMDPs, including continuous problems.
For example, \citet{platt2010belief} simplify planning in large domains by assuming that the most likely observation will always be received, which can provide an acceptable approximation in some problems with unimodal observation distributions.
\citet{morere2016bayesian} solve a monitoring problem with continuous spaces with a Gaussian process belief update.
\citet{hoey2005solving} propose a method for partitioning large observation spaces without information loss, but demonstrate the method only on small state and action spaces that have a modest number of conditional plans.
Other methods involve motion-planning techniques \cite{melchior2007particle,prentice2009belief,bry2011rapidly}.
In particular, \citet{agha2011firm} present a method to take advantage of the existence of a stabilizing controller in belief space planning.
\citet{van2012motion} perform local optimization with respect to uncertainty on a pre-computed path, and \citet{indelman2015planning} devise a hierarchical approach that handles uncertainty in both the robot's state and the surrounding environment.

General purpose online algorithms for POMDPs have also been proposed.
These algorithms are mostly derivatives of the MCTS algorithm for MDPs (\cref{sec:mcts}).
A conceptually straightforward way to solve a POMDP using MCTS is to apply it to the corresponding belief MDP.
Indeed, many tree search techniques have been applied to POMDP problems in this way \cite{ross2008online}.
However, when the Bayesian belief update is used, this approach is computationally expensive.

A major improvement in efficiency came with the introduction of the partially observable Monte Carlo planning (POMCP) algorithm \cite{silver2016mastering}. It is based on UCT, but can tackle problems many times larger than its predecessors because it uses state trajectory simulations, rather than full belief trajectories, to build the tree.
Each node in the POMCP tree corresponds to an action- or observation-terminated history.
At each iteration, a state is sampled from the current belief and simulated through the nodes that match the history as it is generated.
Thus, the belief at each observation-terminated history node is effectively represented by the collection of states for the appropriate time step in the trajectories that match the history. This is similar to using a rejection sampling unweighted particle filter.
It should be noted that when this thesis uses the term POMCP, it refers only to the UCT-based decision-making algorithm called PO-UCT by \citet{silver2016mastering} and not to the belief update scheme that re-uses simulations from the planning stage.

Determinized sparse partially observable tree (DESPOT) is a similar approach that attempts to achieve better performance by analyzing only a small number of random outcomes in the tree \cite{somani2013despot}.
Additionally, an algorithm called adaptive belief tree (ABT) was designed specifically to accommodate changes in the environment without having to replan from scratch \cite{kurniawati2016online}.

Some very recent methods, e.g. QMDP-Net \cite{karkus2017qmdp}, have attempted to solve POMDPs by training recurrent neural networks.

\subsection{QMDP} \label{sec:qmdp}

One POMDP approximation that is particularly useful in many contexts, but known to be sub-optimal for some, is the QMDP approximation.
This approximation is based on an algorithm proposed by \citet{littman1995learning}.
However, in this thesis, it will refer to a broader class of approximations.
Specifically, it refers to any policy where the action maximizes the expected $Q_\text{MDP}$ value for the belief.

Let $Q_\text{MDP}$ be the optimal state-action value function for the fully observable MDP at the core of the POMDP.
Then the expected $Q_\text{MDP}$ value for a belief action pair is $Q_\text{MDP}(b,a) = \int_{s \in \sspace} Q_\text{MDP}(s,a) b(s) \, ds$.
Since it chooses the action that maximizes this value, it is easy to see that the QMDP approximation is the optimal solution to a problem with partial observability on the current step, but that subsequently becomes fully observable.

Because the QMDP approximation assumes that all information about the state of the problem will become available on the next step, it has no incentive for learning about the state, and hence cannot choose to take costly information-gathering actions, even if they are part of the optimal solution to the POMDP.
Nevertheless, the approximation is useful in many domains because it is much easier to compute than the full POMDP solution --- it requires only the solution to the fully observable MDP, which can usually be calculated efficiently using value iteration or another method.
Moreover the QMDP solution has acceptable quality for many domains (see, for example, \cref{tab:experiments}).

\section{Contributions and Outline}

The body of this thesis is laid out in four chapters.
Each contains one or more contributions.

\Cref{chap:uav} analyzes the use of certifiable trusted resolution logic (TRL) alongside approximate optimization in the context of unmanned aerial vehicle collision avoidance.
Specifically, the price of certifiability is quantified by observing the gap between the Pareto fronts for two approaches: (1) using the TRL with varying levels of conservativeness and (2) directly solving the MDP representation of the problem.
Simulation results then demonstrate that this gap can be largely closed without sacrificing certifiability by using the TRL as a safety constraint on the action space and solving the resulting MDP.

\Cref{chap:multilane} considers the effects of modeling uncertainty in a difficult lane changing task for a self-driving car.
Specifically, simulations are used to estimate the Pareto fronts for planning with several approximations of the internal states of other human drivers such as their intentions and dispositions.
While several other researchers have used internal-state-aware planning methods to interact with human drivers in desirable ways, they have not evaluated whether these methods offer a substantial quantitative improvement over conventional approaches.
In a simplified simulated setting, planning with internal states using a POMDP formulation can significantly improve both safety and efficiency simultaneously.
In particular, POMDP planning that includes state uncertainty is evaluated against both overconfident and conservative MDP models with only outcome uncertainty, showing that planning with state uncertainty has a benefit over both.
The experimental method proposed here is also applicable to other cases in which internal-state-aware planning may improve performance.

The benefits of POMDP planning can only be realized with algorithms that can handle the continuous and irregular domains found in the real world.
To that end, \Cref{chap:pomcpow} proposes a pair of new algorithms for solving POMDPs with continuous state, action, and observation spaces.
These algorithms are motivated by analysis and numerical experiments that show that leading online POMDP solvers cannot handle continuous observation spaces.
Previous solvers are shown to exhibit suboptimal behavior, explained by degenerate belief nodes with only a single state particle.
One particular solver is proven to converge to the suboptimal QMDP approximation.
The new algorithms, POMCPOW and PFT-DPW, handle this problem using progressive widening and weighted particle belief representations.
Numerical experiments show that they are able to solve problems where previous methods fail.

\Cref{chap:pomdpsjl} contains a description of the final contribution, which is a software package, POMDPs.jl, that aims to make the advances made in this thesis easier for others to build upon.
The package uses the features of the Julia programming language to provide a convenient and flexible interface for expressing POMDPs.
It also contains many tool for running simulations and has a suite of state-of-the art solvers and tools for writing new ones.

Much of the work in this thesis has been previously published in various venues~\cite{sunberg2017value,sunberg2018pomcpow,ZS-MK-MP:16,egorov2017pomdps}, but each section of the thesis contains important new additions.
